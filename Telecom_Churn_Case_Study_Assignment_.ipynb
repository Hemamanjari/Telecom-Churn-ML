{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbc0c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "# Supressing the warnings generated\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Displaying all Columns without restrictions\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a6fc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "telecom_churn = pd.read_csv(\"telecom_churn_data.csv\")\n",
    "telecom_churn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4b8514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the dimensions of the dataset\n",
    "telecom_churn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e969e08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the informations regarding the dataset\n",
    "telecom_churn.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dfede2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical analysis of the numercial features\n",
    "telecom_churn.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d527855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the columns unique values and drop such columns with its value as 1\n",
    "unique_1_col=[]\n",
    "for i in telecom_churn.columns:\n",
    "    if telecom_churn[i].nunique() == 1:\n",
    "        unique_1_col.append(i)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "telecom_churn.drop(unique_1_col, axis=1, inplace = True)\n",
    "print(\"\\n The following Columns are dropped from the dataset as their unique value is 1. (i.e.)It has no variance in the model\\n\",\n",
    "      unique_1_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500f5697",
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_churn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea5979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkng the overall missing values in the dataset\n",
    "((telecom_churn.isnull().sum()/telecom_churn.shape[0])*100).round(2).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aa8f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since columns with datetime values represented as object, they can be converted into datetime format\n",
    "# selecting all the columns with datetime format\n",
    "date_col= telecom_churn.select_dtypes(include=['object'])\n",
    "print(\"These are the columns available with datetime format represented as object\\n\",date_col.columns)\n",
    "\n",
    "# Converting the selected columns to datetime format\n",
    "for i in date_col.columns:\n",
    "    telecom_churn[i] = pd.to_datetime(telecom_churn[i])\n",
    "\n",
    "# Current dimension of the dataset\n",
    "telecom_churn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483eb473",
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_churn.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da27388",
   "metadata": {},
   "source": [
    "### Handling missing values\n",
    "#### Handling missing values of meaningful attribute column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6295877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values with respect to `data recharge` attributes\n",
    "telecom_churn[['date_of_last_rech_data_6','total_rech_data_6','max_rech_data_6']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b04dada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It can deduced if the total_rech_data and the max_rech_data also has missing values, \n",
    "# the missing values in all the columns mentioned can be considered as meaningful missing. Hence imputing 0 as their values.\n",
    "# Meaningful missing in this case represents the the customer has not done any recharge for mobile interenet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c12fcf9",
   "metadata": {},
   "source": [
    "#### Handling the missing values for the attributes `total_rech_data_*`, `max_rech_data_*` and for month 6, 7, 8 and 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c502d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Code for conditional imputation\n",
    "for i in range(len(telecom_churn)):\n",
    "  # Handling 'total_rech_data', 'max_rech_data' and for month 6\n",
    "    if pd.isnull((telecom_churn['total_rech_data_6'][i]) and (telecom_churn['max_rech_data_6'][i])):\n",
    "        if pd.isnull(telecom_churn['date_of_last_rech_data_6'][i]):\n",
    "            telecom_churn['total_rech_data_6'][i]=0\n",
    "            telecom_churn['max_rech_data_6'][i]=0\n",
    "\n",
    "  # Handling 'total_rech_data', 'max_rech_data' and for month 7\n",
    "    if pd.isnull((telecom_churn['total_rech_data_7'][i]) and (telecom_churn['max_rech_data_7'][i])):\n",
    "        if pd.isnull(telecom_churn['date_of_last_rech_data_7'][i]):\n",
    "            telecom_churn['total_rech_data_7'][i]=0\n",
    "            telecom_churn['max_rech_data_7'][i]=0\n",
    "\n",
    "  # Handling 'total_rech_data', 'max_rech_data' and for month 8\n",
    "    if pd.isnull((telecom_churn['total_rech_data_8'][i]) and (telecom_churn['max_rech_data_8'][i])):\n",
    "        if pd.isnull(telecom_churn['date_of_last_rech_data_8'][i]):\n",
    "            telecom_churn['total_rech_data_8'][i]=0\n",
    "            telecom_churn['max_rech_data_8'][i]=0\n",
    "\n",
    "  # Handling 'total_rech_data', 'max_rech_data' and for month 9\n",
    "    if pd.isnull((telecom_churn['total_rech_data_9'][i]) and (telecom_churn['max_rech_data_9'][i])):\n",
    "        if pd.isnull(telecom_churn['date_of_last_rech_data_9'][i]):\n",
    "            telecom_churn['total_rech_data_9'][i]=0\n",
    "            telecom_churn['max_rech_data_9'][i]=0\n",
    "\n",
    "print(\"The columns 'total_rech_data_*' and 'max_rech_data_*' are imputed with 0 based on the condition explained above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271f6417",
   "metadata": {},
   "source": [
    "#### Handling the missing values for the attributes `count_rech_2g_*`,`count_rech_3g_*` for month 6, 7, 8 and 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a106a1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the related columns values\n",
    "telecom_churn[['count_rech_2g_6','count_rech_3g_6','total_rech_data_6']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6125d6c8",
   "metadata": {},
   "source": [
    "#### From the above tablular the column values of `total_rech_data` for each month from 6 to 9 respectively is the sum of the columns values of `count_rech_2g` for each month from 6 to 9 respectively and `count_rech_3g` for each month from 6 to 9 respectively, which derives to a multicollinearity issue. \n",
    "\n",
    "In order to reduce the multicollinearity, we can drop the columns `count_rech_2g` for each month from 6 to 9 respectively and `count_rech_3g` for each month from 6 to 9 respectively.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e316ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns 'count_rech_2g_*' & 'count_rech_3g_*' for the months 6,7,8 and 9 \n",
    "telecom_churn.drop(['count_rech_2g_6','count_rech_3g_6',\n",
    "                   'count_rech_2g_7','count_rech_3g_7',\n",
    "                   'count_rech_2g_8','count_rech_3g_8',\n",
    "                   'count_rech_2g_9','count_rech_3g_9'],axis=1, inplace=True)\n",
    "\n",
    "print(\"The 'count_rech_2g_6','count_rech_3g_6','count_rech_2g_7','count_rech_3g_7','count_rech_2g_8','count_rech_3g_8','count_rech_2g_9','count_rech_3g_9' columns are dropped as they can be explained from the 'total_rech_data'column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f2f9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The curent dimensions of the dataset\n",
    "telecom_churn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb1f37a",
   "metadata": {},
   "source": [
    "#### Handling the missing values for the attributes `arpu_3g_*`,`arpu_2g_*` for month 6, 7, 8 and 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56020dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the related columns values\n",
    "telecom_churn[['arpu_3g_6','arpu_2g_6','av_rech_amt_data_6']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855a960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the correlation between the above mentioned columns in tabular for months 6,7,8 and 9\n",
    "print(\"Correlation table for month 6\\n\\n\", telecom_churn[['arpu_3g_6','arpu_2g_6','av_rech_amt_data_6']].corr())\n",
    "print(\"\\nCorrelation table for month 7\\n\\n\", telecom_churn[['arpu_3g_7','arpu_2g_7','av_rech_amt_data_7']].corr())\n",
    "print(\"\\nCorrelation table for month 8\\n\\n\", telecom_churn[['arpu_3g_8','arpu_2g_8','av_rech_amt_data_8']].corr())\n",
    "print(\"\\nCorrelation table for month 9\\n\\n\", telecom_churn[['arpu_3g_9','arpu_2g_9','av_rech_amt_data_9']].corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fd6905",
   "metadata": {},
   "source": [
    "##### From the above correlation table between attributes `arpu_2g_*` and `arpu_3g_*` for each month from 6 to 9 respectively is highly correlated to the attribute `av_rech_amt_data_*` for each month from 6 to 9 respectively.<br> Considering the high correlation between them, it is safer to drop the attributes `arpu_2g_*` and `arpu_3g_*`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02498e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns 'arpu_3g_*'&'arpu_2g_*' in month 6,7,8 and 9 datafrom the dataset\n",
    "telecom_churn.drop(['arpu_3g_6','arpu_2g_6',\n",
    "                  'arpu_3g_7','arpu_2g_7',\n",
    "                  'arpu_3g_8','arpu_2g_8',\n",
    "                  'arpu_3g_9','arpu_2g_9'],axis=1, inplace=True)\n",
    "print(\"\\nThe columns'arpu_3g_6','arpu_2g_6','arpu_3g_7','arpu_2g_7','arpu_3g_8','arpu_2g_8','arpu_3g_9','arpu_2g_9' are dropped from the dataset due to high corellation between their respective arpu_* variable in the dataset\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58aa91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_churn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877774be",
   "metadata": {},
   "source": [
    "#### Handling the other attributes with higher missing value percentage\n",
    "##### The column `fb_user_*` and `night_pck_user_*` for each month from 6 to 9 respectively has a missing values above 50% and does not seem to add any information to understand the data. Hence we can drop these columns for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d32cb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_churn.drop(['fb_user_6','fb_user_7','fb_user_8','fb_user_9',\n",
    "                  'night_pck_user_6','night_pck_user_7','night_pck_user_8','night_pck_user_9'],\n",
    "                  axis=1, inplace=True)\n",
    "print(\"\\nThe columns 'fb_user_6','fb_user_7','fb_user_8','fb_user_9','night_pck_user_6','night_pck_user_7','night_pck_user_8','night_pck_user_9' are dropped from the dataset as it has no meaning to the data snd has high missing values above 50%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd27e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_churn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8186dff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the related columns values\n",
    "telecom_churn[['av_rech_amt_data_7','max_rech_data_7','total_rech_data_7']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7dc72a",
   "metadata": {},
   "source": [
    "#### From the above tabular it is deduced that the missing values for the column `av_rech_amt_data_*` for each month from 6 to 9 can be replaced as 0 if the `total_rech_data_*` for each month from 6 to 9 respectively is 0. i.e. if the total recharge done is 0 then the average recharge amount shall also be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85377924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for conditional imputation\n",
    "\n",
    "for i in range(len(telecom_churn)):\n",
    "  # Handling `av_rech_amt_data`  for month 6\n",
    "    if (pd.isnull(telecom_churn['av_rech_amt_data_6'][i]) and (telecom_churn['total_rech_data_6'][i]==0)):\n",
    "        telecom_churn['av_rech_amt_data_6'][i] = 0\n",
    "\n",
    "  # Handling `av_rech_amt_data`  for month 7\n",
    "    if (pd.isnull(telecom_churn['av_rech_amt_data_7'][i]) and (telecom_churn['total_rech_data_7'][i]==0)):\n",
    "        telecom_churn['av_rech_amt_data_7'][i] = 0\n",
    "\n",
    "  # Handling `av_rech_amt_data`  for month 8\n",
    "    if (pd.isnull(telecom_churn['av_rech_amt_data_8'][i]) and (telecom_churn['total_rech_data_8'][i]==0)):\n",
    "        telecom_churn['av_rech_amt_data_8'][i] = 0\n",
    "\n",
    "  # Handling `av_rech_amt_data`  for month 9\n",
    "    if (pd.isnull(telecom_churn['av_rech_amt_data_9'][i]) and (telecom_churn['total_rech_data_9'][i]==0)):\n",
    "        telecom_churn['av_rech_amt_data_9'][i] = 0\n",
    "\n",
    "\n",
    "print(\"The columns 'av_rech_amt_data_*', are imputed with 0 based on the condition explained above\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b450cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the overall missing values in the dataset\n",
    "((telecom_churn.isnull().sum()/telecom_churn.shape[0])*100).round(2).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ea5cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_churn.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251569a0",
   "metadata": {},
   "source": [
    "#### From the above results, we can conclude, the `date_of_last_rech_data_*` corresponding to months 6,7,8 and 9 are of no value after the conditional imputation of of columns `total_rech_data_*`,` max_rech_data_*`are completes.<br>Also the missing value percentage is high for these columns and can be dropped from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc7ba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns related to datetime dtype from the dataset\n",
    "telecom_churn.drop([\"date_of_last_rech_data_6\",\"date_of_last_rech_data_7\",\n",
    "                   \"date_of_last_rech_data_8\",\"date_of_last_rech_data_9\"], axis=1, inplace=True)\n",
    "print(\"\\nThe columns 'date_of_last_rech_data_6','date_of_last_rech_data_7','date_of_last_rech_data_8','date_of_last_rech_data_9' are dropped as it has no significance to the data\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0251dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since datetime column not of much use, we can drop the `date_of_last_rech_data_*` column for all months\n",
    "# Dropping the columns related to datetime dtype from the dataset\n",
    "telecom_churn.drop([\"date_of_last_rech_6\",\"date_of_last_rech_7\",\n",
    "                   \"date_of_last_rech_8\",\"date_of_last_rech_9\"], axis=1, inplace=True)\n",
    "print(\"\\nThe columns 'date_of_last_rech_6','date_of_last_rech_7','date_of_last_rech_8','date_of_last_rech_9' are dropped as it has no significance to the data\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55365888",
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_churn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e55987",
   "metadata": {},
   "source": [
    "### Filtering the High Value Customer from Good Phase\n",
    "#### Since the columns used to determine the High Value Customer is clear of null values, we can filter the overall data and then handle the remaining missing values for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c0ca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the data \n",
    "# We are filtering the data in accordance to total revenue generated per customer.\n",
    "\n",
    "# first we need the total amount recharge amount done for data alone, we have average rechage amount done. \n",
    "\n",
    "# Calculating the total recharge amount done for data alone in months 6,7,8 and 9\n",
    "telecom_churn['total_rech_amt_data_6']=telecom_churn['av_rech_amt_data_6'] * telecom_churn['total_rech_data_6']\n",
    "telecom_churn['total_rech_amt_data_7']=telecom_churn['av_rech_amt_data_7'] * telecom_churn['total_rech_data_7']\n",
    "\n",
    "# Calculating the overall recharge amount for the months 6,7,8 and 9\n",
    "telecom_churn['overall_rech_amt_6'] = telecom_churn['total_rech_amt_data_6'] + telecom_churn['total_rech_amt_6']\n",
    "telecom_churn['overall_rech_amt_7'] = telecom_churn['total_rech_amt_data_7'] + telecom_churn['total_rech_amt_7']\n",
    "\n",
    "# Calculating the average recharge done by customer in months June and July(i.e. 6th and 7th month)\n",
    "telecom_churn['avg_rech_amt_6_7'] = (telecom_churn['overall_rech_amt_6'] + telecom_churn['overall_rech_amt_7'])/2\n",
    "\n",
    "# Finding the value of 70th percentage in the overall revenues defining the high value customer creteria for the company\n",
    "cut_off = telecom_churn['avg_rech_amt_6_7'].quantile(0.70)\n",
    "print(\"\\nThe 70th quantile value to determine the High Value Customer is: \",cut_off,\"\\n\")\n",
    "\n",
    "# Filtering the data to the top 30% considered as High Value Customer\n",
    "telecom_churn = telecom_churn[telecom_churn['avg_rech_amt_6_7'] >= cut_off]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04ce161",
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_churn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f5b360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The total number of customers is now limited to ~30k who lies under the High Value customer \n",
    "# criteria based upon which the model is built. \n",
    "# Let us check the missing values percentages again for the HVC group\n",
    "# Checkng the overall missing values in the dataset\n",
    "    \n",
    "((telecom_churn.isnull().sum()/telecom_churn.shape[0])*100).round(2).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49833ab6",
   "metadata": {},
   "source": [
    "#### The remaining attributes with missing value can be imputed using the advanced imputation technique like `KNNImputer`.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242c4cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical columns available\n",
    "num_col = telecom_churn.select_dtypes(include = ['int64','float64']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e1b978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries for Scaling and Imputation\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Calling the Scaling function\n",
    "scalar = MinMaxScaler()\n",
    "\n",
    "# Scaling and transforming the data for the columns that are numerical\n",
    "telecom_churn[num_col]=scalar.fit_transform(telecom_churn[num_col])\n",
    "\n",
    "# Calling the KNN Imputer function\n",
    "knn=KNNImputer(n_neighbors=3)\n",
    "\n",
    "# Imputing the NaN values using KNN Imputer\n",
    "start_time=time.time()\n",
    "\n",
    "telecom_churn_knn = pd.DataFrame(knn.fit_transform(telecom_churn[num_col]))\n",
    "telecom_churn_knn.columns=telecom_churn[num_col].columns\n",
    "\n",
    "end_time=time.time()\n",
    "print(\"\\nExecution Time = \", round(end_time-start_time,2),\"seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1693a223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for any null values after imputation for numerical columns\n",
    "telecom_churn_knn.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390bd032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The KNN Imputer has replaced all the null values in the numerical column usingK-means algorithm sucessfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45095fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we sclaed the numerical columns for the purpose of handling the null values, \n",
    "    #we can restore the sclaed alues to its original form.\n",
    "\n",
    "# Converting the scaled data back to the original data\n",
    "telecom_churn[num_col]=scalar.inverse_transform(telecom_churn_knn)\n",
    "\n",
    "# Checking the top 10 data\n",
    "telecom_churn.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ccf00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the overall missing values in the dataset\n",
    "((telecom_churn.isnull().sum()/telecom_churn.shape[0])*100).round(2).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455765c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_churn.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ae3a69",
   "metadata": {},
   "source": [
    "#### Defining Churn Variable  As explained above in the introduction, we are deriving based on usage based for this model.\n",
    "\n",
    "#### For that, we need to find the derive churn variable using `total_ic_mou_9`,`total_og_mou_9`,`vol_2g_mb_9` and `vol_3g_mb_9` attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaac277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the columns to define churn variable (i.e. TARGET Variable)\n",
    "churn_col=['total_ic_mou_9','total_og_mou_9','vol_2g_mb_9','vol_3g_mb_9']\n",
    "telecom_churn[churn_col].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3678ba9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Initializing the churn variable.\n",
    "telecom_churn['churn']=0\n",
    "\n",
    "# Imputing the churn values based on the condition\n",
    "telecom_churn['churn'] = np.where(telecom_churn[churn_col].sum(axis=1) == 0, 1, 0)\n",
    "telecom_churn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f69499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# lets find out churn/non churn percentage\n",
    "print((telecom_churn['churn'].value_counts()/len(telecom_churn))*100)\n",
    "((telecom_churn['churn'].value_counts()/len(telecom_churn))*100).plot(kind=\"pie\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ade34b",
   "metadata": {},
   "source": [
    "#### There is a problem of class imbalance which needs to be taken care of\n",
    "#### Since this variable `churn` is the target variable, all the columns relating to this variable(i.e. all columns with suffix `_9`) can be dropped from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a2c998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting all the churn phase columns in order to drop \n",
    "\n",
    "churn_phase_cols = [col for col in telecom_churn.columns if '_9' in col]\n",
    "print(\"The columns from churn phase are:\\n\",churn_phase_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aa40fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the selected churn phase columns\n",
    "telecom_churn.drop(churn_phase_cols, axis=1, inplace=True)\n",
    "\n",
    "# The curent dimension of the dataset after dropping the churn related columns\n",
    "telecom_churn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b3ce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_churn.drop(['total_rech_amt_data_6','av_rech_amt_data_6',\n",
    "                   'total_rech_data_6','total_rech_amt_6',\n",
    "                  'total_rech_amt_data_7','av_rech_amt_data_7',\n",
    "                   'total_rech_data_7','total_rech_amt_7'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e515101",
   "metadata": {},
   "source": [
    "#### We can also create new columns for the defining the good phase variables and drop the seperate 6th and 7 month variables\n",
    "#### Before proceding to check the remaining missing value handling, let us check the collineartity of the indepedent variables and try to understand their dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4311d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of column names for each month\n",
    "mon_6_cols = [col for col in telecom_churn.columns if '_6' in col]\n",
    "mon_7_cols = [col for col in telecom_churn.columns if '_7' in col]\n",
    "mon_8_cols = [col for col in telecom_churn.columns if '_8' in col]\n",
    "\n",
    "\n",
    "# lets check the correlation amongst the independent variables, drop the highly correlated ones\n",
    "telecom_churn_corr = telecom_churn.corr()\n",
    "telecom_churn_corr.loc[:,:] = np.tril(telecom_churn_corr, k=-1)\n",
    "telecom_churn_corr = telecom_churn_corr.stack()\n",
    "telecom_churn_corr\n",
    "telecom_churn_corr[(telecom_churn_corr > 0.80) | (telecom_churn_corr < -0.80)].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd772965",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_drop=['total_rech_amt_8','isd_og_mou_8','isd_og_mou_7','sachet_2g_8','total_ic_mou_6',\n",
    "            'total_ic_mou_8','total_ic_mou_7','std_og_t2t_mou_6','std_og_t2t_mou_8','std_og_t2t_mou_7',\n",
    "            'std_og_t2m_mou_7','std_og_t2m_mou_8',]\n",
    "\n",
    "# These columns can be dropped as they are highly collinered with other predictor variables.\n",
    "# criteria set is for collinearity of 85%\n",
    "\n",
    "#  dropping these column\n",
    "telecom_churn.drop(col_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e91f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_churn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6497147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a column called 'aon', we can derive new variables from this to explain the data w.r.t churn.\n",
    "\n",
    "# creating a new variable 'tenure'\n",
    "telecom_churn['tenure'] = (telecom_churn['aon']/30).round(0)\n",
    "\n",
    "# Since we derived a new column 'tenure' from 'aon', we can drop it\n",
    "telecom_churn.drop('aon',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cf4372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the distribution of the tenure variable\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.distplot(telecom_churn['tenure'],bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725f44a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_range = [0, 6, 12, 24, 60, 61]\n",
    "tn_label = [ '0-6 Months', '6-12 Months', '1-2 Yrs', '2-5 Yrs', '5 Yrs and above']\n",
    "telecom_churn['tenure_range'] = pd.cut(telecom_churn['tenure'], tn_range, labels=tn_label)\n",
    "telecom_churn['tenure_range'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d2b9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a bar plot for tenure range\n",
    "sns.barplot(x='tenure_range',y='churn', data=telecom_churn)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea347ed",
   "metadata": {},
   "source": [
    "#### It can be seen that the maximum churn rate happens within 0-6 month, but it gradually decreases as the customer retains in the network.\n",
    "#### The average revenue per user is good phase of customer is given by arpu_6 and arpu_7. since we have two seperate averages, lets take an average to these two and drop the other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad128f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_churn[\"avg_arpu_6_7\"]= (telecom_churn['arpu_6']+telecom_churn['arpu_7'])/2\n",
    "telecom_churn['avg_arpu_6_7'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4f74c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets drop the original columns as they are derived to a new column for better understanding of the data\n",
    "\n",
    "telecom_churn.drop(['arpu_6','arpu_7'], axis=1, inplace=True)\n",
    "\n",
    "# The curent dimension of the dataset after dropping few unwanted columns\n",
    "telecom_churn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bb2848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the column created\n",
    "sns.distplot(telecom_churn['avg_arpu_6_7'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af08918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Correlation between target variable(SalePrice) with the other variable in the dataset\n",
    "#plt.figure(figsize=(10,50))\n",
    "heatmap_churn = sns.heatmap(telecom_churn.corr()[['churn']].sort_values(ascending=False, by='churn'),annot=True, \n",
    "                                cmap='summer')\n",
    "heatmap_churn.set_title(\"Features Correlating with Churn variable\", fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f655fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Avg Outgoing Calls & calls on romaning for 6 & 7th months are positively correlated with churn.\n",
    "### Avg Revenue, No. Of Recharge for 8th month has negative correlation with churn.\n",
    "\n",
    "# lets now draw a scatter plot between total recharge and avg revenue for the 8th month\n",
    "telecom_churn[['total_rech_num_8', 'arpu_8']].plot.scatter(x = 'total_rech_num_8',\n",
    "                                                              y='arpu_8')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721a0a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x = telecom_churn.churn, y = telecom_churn.tenure)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e94866",
   "metadata": {},
   "outputs": [],
   "source": [
    "## From the above plot , its clear tenured customers do no churn and they keep availing telecom services**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6aeb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot between churn vs max rechare amount\n",
    "ax = sns.kdeplot(telecom_churn.max_rech_amt_8[(telecom_churn[\"churn\"] == 0)],\n",
    "                color=\"Red\", shade = True)\n",
    "ax = sns.kdeplot(telecom_churn.max_rech_amt_8[(telecom_churn[\"churn\"] == 1)],\n",
    "                ax =ax, color=\"Blue\", shade= True)\n",
    "ax.legend([\"No-Churn\",\"Churn\"],loc='upper right')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_xlabel('Volume based cost')\n",
    "ax.set_title('Distribution of Max Recharge Amount by churn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135ecf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# churn vs max rechare amount\n",
    "ax = sns.kdeplot(telecom_churn.av_rech_amt_data_8[(telecom_churn[\"churn\"] == 0)],\n",
    "                color=\"Red\", shade = True)\n",
    "ax = sns.kdeplot(telecom_churn.av_rech_amt_data_8[(telecom_churn[\"churn\"] == 1)],\n",
    "                ax =ax, color=\"Blue\", shade= True)\n",
    "ax.legend([\"No-Churn\",\"Churn\"],loc='upper right')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_xlabel('Volume based cost')\n",
    "ax.set_title('Distribution of Average Recharge Amount for Data by churn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257f9d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating categories for month 8 column totalrecharge and their count\n",
    "telecom_churn['total_rech_data_group_8']=pd.cut(telecom_churn['total_rech_data_8'],[-1,0,10,25,100],labels=[\"No_Recharge\",\"<=10_Recharges\",\"10-25_Recharges\",\">25_Recharges\"])\n",
    "telecom_churn['total_rech_num_group_8']=pd.cut(telecom_churn['total_rech_num_8'],[-1,0,10,25,1000],labels=[\"No_Recharge\",\"<=10_Recharges\",\"10-25_Recharges\",\">25_Recharges\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511a1c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "#import matplotlib as plt\n",
    "#plt.figure(figsize=[12,4])\n",
    "sns.countplot(data=telecom_churn,x=\"total_rech_data_group_8\",hue=\"churn\")\n",
    "print(\"\\t\\t\\t\\t\\tDistribution of total_rech_data_8 variable\\n\",telecom_churn['total_rech_data_group_8'].value_counts())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4148c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=[12,4])\n",
    "sns.countplot(data=telecom_churn,x=\"total_rech_num_group_8\",hue=\"churn\")\n",
    "print(\"\\t\\t\\t\\t\\tDistribution of total_rech_num_8 variable\\n\",telecom_churn['total_rech_num_group_8'].value_counts())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c1de9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As number of recharge rate increases, the churn rate decreases clearly.\n",
    "# Creating a dummy variable for some of the categorical variables and dropping the first one.\n",
    "dummy = pd.get_dummies(telecom_churn[['total_rech_data_group_8','total_rech_num_group_8','tenure_range']], drop_first=True)\n",
    "dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26976960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the results to the master dataframe\n",
    "telecom_churn = pd.concat([telecom_churn, dummy], axis=1)\n",
    "telecom_churn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cba4ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the filtered dataframe\n",
    "\n",
    "df=telecom_churn[:].copy()\n",
    "\n",
    "# Dropping unwanted columns\n",
    "df.drop(['tenure_range','mobile_number','total_rech_data_group_8','total_rech_num_group_8','sep_vbc_3g','tenure'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bafb7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30d08a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create X dataset for model building.\n",
    "X = df.drop(['churn'],axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fbb135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create y dataset for model building.\n",
    "y=df['churn']\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7000b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dateset into train and test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, train_size=0.7, random_state=1)\n",
    "print(\"Dimension of X_train:\", X_train.shape)\n",
    "print(\"Dimension of X_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c33ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4c8d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col = X_train.select_dtypes(include = ['int64','float64']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af05344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply scaling on the dataset\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train[num_col] = scaler.fit_transform(X_train[num_col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8601e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fb4f73",
   "metadata": {},
   "source": [
    "### Data Imbalance Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79474371",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using SMOTE method, we can balance the data w.r.t. churn variable and proceed further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969a8e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install imblearn.over_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef609f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn import under_sampling, over_sampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_sm,y_train_sm = sm.fit_resample(X_train,y_train)\n",
    "\n",
    "\n",
    "print(\"Dimension of X_train_sm Shape:\", X_train_sm.shape)\n",
    "print(\"Dimension of y_train_sm Shape:\", y_train_sm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd82cb49",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b539fbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for Model creation\n",
    "import statsmodels.api as sm\n",
    "# Logistic regression model\n",
    "logm1 = sm.GLM(y_train_sm,(sm.add_constant(X_train_sm)), family = sm.families.Binomial())\n",
    "logm1.fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6371681d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression using Feature Selection (RFE method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a822e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# running RFE with 20 variables as output\n",
    "\n",
    "rfe = RFE(logreg,step=20) \n",
    "\n",
    "rfe = rfe.fit(X_train_sm, y_train_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc6f8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_columns=X_train_sm.columns[rfe.support_]\n",
    "print(\"The selected columns by RFE for modelling are: \\n\\n\",rfe_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc84582",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(X_train_sm.columns, rfe.support_, rfe.ranking_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19196cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Assessing the model with StatsModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6681c21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_SM = sm.add_constant(X_train_sm[rfe_columns])\n",
    "logm2 = sm.GLM(y_train_sm,X_train_SM, family = sm.families.Binomial())\n",
    "res = logm2.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b3ba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the p-value of the individual columns, \n",
    "    # we can drop the column 'loc_ic_t2t_mou_8' as it has high p-value of 0.80\n",
    "rfe_columns_1=rfe_columns.drop('loc_ic_t2t_mou_8',1)\n",
    "print(\"\\nThe new set of edited featured are:\\n\",rfe_columns_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e984c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model with the edited feature list\n",
    "X_train_SM = sm.add_constant(X_train_sm[rfe_columns_1])\n",
    "logm2 = sm.GLM(y_train_sm,X_train_SM, family = sm.families.Binomial())\n",
    "res = logm2.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7af3639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the p-value of the individual columns, \n",
    "    # we can drop the column 'loc_ic_t2m_mou_8' as it has high p-value of 0.80\n",
    "rfe_columns_2=rfe_columns_1.drop('loc_ic_t2m_mou_8',1)\n",
    "print(\"\\nThe new set of edited featured are:\\n\",rfe_columns_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eec2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model with the edited feature list\n",
    "X_train_SM = sm.add_constant(X_train_sm[rfe_columns_2])\n",
    "logm2 = sm.GLM(y_train_sm,X_train_SM, family = sm.families.Binomial())\n",
    "res = logm2.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6f889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted values on the train set\n",
    "y_train_sm_pred = res.predict(X_train_SM)\n",
    "y_train_sm_pred = y_train_sm_pred.values.reshape(-1)\n",
    "y_train_sm_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5116df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Creating a dataframe with the actual churn flag and the predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b788cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sm_pred_final = pd.DataFrame({'Converted':y_train_sm.values, 'Converted_prob':y_train_sm_pred})\n",
    "y_train_sm_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8302c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Creating new column 'churn_pred' with 1 if Churn_Prob > 0.5 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1645e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sm_pred_final['churn_pred'] = y_train_sm_pred_final.Converted_prob.map(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "# Viewing the prediction results\n",
    "y_train_sm_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed697a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Confusion matrix \n",
    "confusion = metrics.confusion_matrix(y_train_sm_pred_final.Converted, y_train_sm_pred_final.churn_pred )\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485fb721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted     not_churn    churn\n",
    "# Actual\n",
    "# not_churn        15661      3627\n",
    "# churn            2775       16513  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1eeaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the overall accuracy.\n",
    "print(\"The overall accuracy of the model is:\",metrics.accuracy_score(y_train_sm_pred_final.Converted, y_train_sm_pred_final.churn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8271224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the VIF values of the feature variables. \n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827a0411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train_sm[rfe_columns_2].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train_sm[rfe_columns].values, i) for i in range(X_train_sm[rfe_columns_2].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de96b0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics\n",
    "TP = confusion[1,1] # true positive \n",
    "TN = confusion[0,0] # true negatives\n",
    "FP = confusion[0,1] # false positives\n",
    "FN = confusion[1,0] # false negatives\n",
    "\n",
    "\n",
    "# Let's see the sensitivity of our logistic regression model\n",
    "print(\"Sensitivity = \",TP / float(TP+FN))\n",
    "\n",
    "# Let us calculate specificity\n",
    "print(\"Specificity = \",TN / float(TN+FP))\n",
    "\n",
    "# Calculate false postive rate - predicting churn when customer does not have churned\n",
    "print(\"False Positive Rate = \",FP/ float(TN+FP))\n",
    "\n",
    "# positive predictive value \n",
    "print (\"Precision = \",TP / float(TP+FP))\n",
    "\n",
    "# Negative predictive value\n",
    "print (\"True Negative Prediction Rate = \",TN / float(TN+ FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc61f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad91a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Defining a function to plot the roc curve\n",
    "def draw_roc( actual, probs ):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n",
    "                                              drop_intermediate = False )\n",
    "    auc_score = metrics.roc_auc_score( actual, probs )\n",
    "    plt.figure(figsize=[5, 5])\n",
    "    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate or [1 - True Negative Prediction Rate]')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db27275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the variables to plot the curve\n",
    "fpr, tpr, thresholds = metrics.roc_curve( y_train_sm_pred_final.Converted, y_train_sm_pred_final.Converted_prob, drop_intermediate = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ec4dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the curve for the obtained metrics\n",
    "draw_roc(y_train_sm_pred_final.Converted, y_train_sm_pred_final.Converted_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21afc641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To Find the Optimal Cut off Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1813408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create columns with different probability cutoffs \n",
    "numbers = [float(x)/10 for x in range(10)]\n",
    "for i in numbers:\n",
    "    y_train_sm_pred_final[i]= y_train_sm_pred_final.Converted_prob.map(lambda x: 1 if x > i else 0)\n",
    "y_train_sm_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df43397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\n",
    "cutoff_df = pd.DataFrame( columns = ['probability','accuracy','sensitivity','specificity'])\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# TP = confusion[1,1] # true positive \n",
    "# TN = confusion[0,0] # true negatives\n",
    "# FP = confusion[0,1] # false positives\n",
    "# FN = confusion[1,0] # false negatives\n",
    "\n",
    "num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "for i in num:\n",
    "    cm1 = metrics.confusion_matrix(y_train_sm_pred_final.Converted, y_train_sm_pred_final[i] )\n",
    "    total1=sum(sum(cm1))\n",
    "    accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
    "    \n",
    "    specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    cutoff_df.loc[i] =[ i ,accuracy,sensitivity,specificity]\n",
    "print(cutoff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1610fdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy sensitivity and specificity for various probabilities calculated above.\n",
    "cutoff_df.plot.line(x='probability', y=['accuracy','sensitivity','specificity'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e988790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initially we selected the optimm point of classification as 0.5.\n",
    "# From the above graph, we can see the optimum cutoff is slightly higher than 0.5 but lies lower than 0.6. So lets tweek a little more within this range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c217ea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create columns with refined probability cutoffs \n",
    "numbers = [0.50,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59]\n",
    "for i in numbers:\n",
    "    y_train_sm_pred_final[i]= y_train_sm_pred_final.Converted_prob.map(lambda x: 1 if x > i else 0)\n",
    "y_train_sm_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2215c32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\n",
    "cutoff_df = pd.DataFrame( columns = ['probability','accuracy','sensitivity','specificity'])\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# TP = confusion[1,1] # true positive \n",
    "# TN = confusion[0,0] # true negatives\n",
    "# FP = confusion[0,1] # false positives\n",
    "# FN = confusion[1,0] # false negatives\n",
    "\n",
    "num = [0.50,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59]\n",
    "for i in num:\n",
    "    cm1 = metrics.confusion_matrix(y_train_sm_pred_final.Converted, y_train_sm_pred_final[i] )\n",
    "    total1=sum(sum(cm1))\n",
    "    accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
    "    \n",
    "    specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    cutoff_df.loc[i] =[ i ,accuracy,sensitivity,specificity]\n",
    "print(cutoff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8059bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy sensitivity and specificity for various probabilities calculated above.\n",
    "cutoff_df.plot.line(x='probability', y=['accuracy','sensitivity','specificity'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b3eb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the above graph we can conclude, the optimal cutoff point in the probability to define the predicted churn variabe converges at `0.53'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d74a319",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### From the curve above, 0.2 is the optimum point to take it as a cutoff probability.\n",
    "\n",
    "y_train_sm_pred_final['final_churn_pred'] = y_train_sm_pred_final.Converted_prob.map( lambda x: 1 if x > 0.54 else 0)\n",
    "\n",
    "y_train_sm_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffacb5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the ovearall accuracy again\n",
    "print(\"The overall accuracy of the model now is:\",metrics.accuracy_score(y_train_sm_pred_final.Converted, y_train_sm_pred_final.final_churn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefb1e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion2 = metrics.confusion_matrix(y_train_sm_pred_final.Converted, y_train_sm_pred_final.final_churn_pred )\n",
    "print(confusion2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046ffd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP2 = confusion2[1,1] # true positive \n",
    "TN2 = confusion2[0,0] # true negatives\n",
    "FP2 = confusion2[0,1] # false positives\n",
    "FN2 = confusion2[1,0] # false negatives\n",
    "\n",
    "# Let's see the sensitivity of our logistic regression model\n",
    "print(\"Sensitivity = \",TP2 / float(TP2+FN2))\n",
    "\n",
    "# Let us calculate specificity\n",
    "print(\"Specificity = \",TN2 / float(TN2+FP2))\n",
    "\n",
    "# Calculate false postive rate - predicting churn when customer does not have churned\n",
    "print(\"False Positive Rate = \",FP2/ float(TN2+FP2))\n",
    "\n",
    "# positive predictive value \n",
    "print (\"Precision = \",TP2 / float(TP2+FP2))\n",
    "\n",
    "# Negative predictive value\n",
    "print (\"True Negative Prediction Rate = \",TN2 / float(TN2 + FN2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18f2286",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision and Recall Tradeoff\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "p, r, thresholds = precision_recall_curve(y_train_sm_pred_final.Converted, y_train_sm_pred_final.Converted_prob)\n",
    "\n",
    "# Plotting the curve\n",
    "plt.plot(thresholds, p[:-1], \"g-\")\n",
    "plt.plot(thresholds, r[:-1], \"r-\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5f285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Making the predictions on the test data\n",
    "## Transforming and feature selection for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207005d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the test data\n",
    "X_test[num_col] = scaler.transform(X_test[num_col])\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89803884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "X_test=X_test[rfe_columns_2]\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17263e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding constant to the test model.\n",
    "X_test_SM = sm.add_constant(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eb83d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = res.predict(X_test_SM)\n",
    "print(\"\\n The first ten probability value of the prediction are:\\n\",y_test_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c25460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.DataFrame(y_test_pred)\n",
    "y_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a706beaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=y_pred.rename(columns = {0:\"Conv_prob\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e432228a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_df = pd.DataFrame(y_test)\n",
    "y_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3206286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final = pd.concat([y_test_df,y_pred],axis=1)\n",
    "y_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d7c37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final['test_churn_pred'] = y_pred_final.Conv_prob.map(lambda x: 1 if x>0.54 else 0)\n",
    "y_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b517fec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the overall accuracy of the predicted set.\n",
    "metrics.accuracy_score(y_pred_final.churn, y_pred_final.test_churn_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ac356e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metric Evaluation\n",
    "\n",
    "# Confusion Matrix\n",
    "confusion2_test = metrics.confusion_matrix(y_pred_final.churn, y_pred_final.test_churn_pred)\n",
    "print(\"Confusion Matrix\\n\",confusion2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7c1a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating model validation parameters\n",
    "TP3 = confusion2_test[1,1] # true positive \n",
    "TN3 = confusion2_test[0,0] # true negatives\n",
    "FP3 = confusion2_test[0,1] # false positives\n",
    "FN3 = confusion2_test[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d55fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the sensitivity of our logistic regression model\n",
    "print(\"Sensitivity = \",TP3 / float(TP3+FN3))\n",
    "\n",
    "# Let us calculate specificity\n",
    "print(\"Specificity = \",TN3 / float(TN3+FP3))\n",
    "\n",
    "# Calculate false postive rate - predicting churn when customer does not have churned\n",
    "print(\"False Positive Rate = \",FP3/ float(TN3+FP3))\n",
    "\n",
    "# positive predictive value \n",
    "print (\"Precision = \",TP3 / float(TP3+FP3))\n",
    "\n",
    "# Negative predictive value\n",
    "print (\"True Negative Prediction Rate = \",TN3 / float(TN3+FN3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c6c6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inferences derived\n",
    "\n",
    "print(\"The accuracy of the predicted model is: \",round(metrics.accuracy_score(y_pred_final.churn, y_pred_final.test_churn_pred),2)*100,\"%\")\n",
    "print(\"The sensitivity of the predicted model is: \",round(TP3 / float(TP3+FN3),2)*100,\"%\")\n",
    "\n",
    "print(\"\\nAs the model created is based on a sentivity model, i.e. the True positive rate is given more importance as the actual and prediction of churn by a customer\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3471c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve for the test dataset\n",
    "\n",
    "# Defining the variables to plot the curve\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_pred_final.churn,y_pred_final.Conv_prob, drop_intermediate = False )\n",
    "# Plotting the curve for the obtained metrics\n",
    "draw_roc(y_pred_final.churn,y_pred_final.Conv_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11a4625",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The AUC score for train dataset is 0.90 and the test dataset is 0.88. This model can be considered as a good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd175b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Logistic Regression using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c06aab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dateset into train and test datasets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, train_size=0.7, random_state=1)\n",
    "print(\"Dimension of X_train:\", X_train.shape)\n",
    "print(\"Dimension of X_test:\", X_test.shape)\n",
    "\n",
    "# apply scaling on the dataset\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train[num_col] = scaler.fit_transform(X_train[num_col])\n",
    "X_test[num_col] = scaler.transform(X_test[num_col])\n",
    "\n",
    "# Applying SMOTE technique for data imbalance correction\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_sm,y_train_sm = sm.fit_resample(X_train,y_train)\n",
    "print(\"Dimension of X_train_sm Shape:\", X_train_sm.shape)\n",
    "print(\"Dimension of y_train_sm Shape:\", y_train_sm.shape)\n",
    "\n",
    "X_train_sm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609b3ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(random_state=42)\n",
    "\n",
    "# applying PCA on train data\n",
    "pca.fit(X_train_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106835c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sm_pca=pca.fit_transform(X_train_sm)\n",
    "print(\"Dimension of X_train_sm_pca: \",X_train_sm_pca.shape)\n",
    "\n",
    "X_test_pca=pca.transform(X_test)\n",
    "print(\"Dimension of X_test_pca: \",X_test_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a79b541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viewing the PCA components\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cc47dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "logreg_pca = LogisticRegression()\n",
    "logreg_pca.fit(X_train_sm_pca, y_train_sm)\n",
    "\n",
    "# making the predictions\n",
    "y_pred = logreg_pca.predict(X_test_pca)\n",
    "\n",
    "# converting the prediction into a dataframe\n",
    "y_pred_df = pd.DataFrame(y_pred)\n",
    "print(\"Dimension of y_pred_df:\", y_pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19926b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Checking the Confusion matrix\n",
    "print(\"Confusion Matirx for y_test & y_pred\\n\",confusion_matrix(y_test,y_pred),\"\\n\")\n",
    "\n",
    "# Checking the Accuracy of the Predicted model.\n",
    "print(\"Accuracy of the logistic regression model with PCA: \",accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add02c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(1,len(pca.explained_variance_ratio_)+1),pca.explained_variance_ratio_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b39e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_cumu = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Making a scree plot\n",
    "fig = plt.figure(figsize=[12,7])\n",
    "plt.plot(var_cumu)\n",
    "plt.xlabel('no of principal components')\n",
    "plt.ylabel('explained variance - cumulative')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52b5c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(np.round(pca.explained_variance_ratio_, decimals=3)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9839320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#90% of the data can be explained with 8 PCA components\n",
    "    \n",
    "#Lets Fit the dataset with the 8 explainable components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ea0464",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_8 = PCA(n_components=15)\n",
    "\n",
    "train_pca_8 = pca_8.fit_transform(X_train_sm)\n",
    "print(\"Dimension for Train dataset using PCA: \", train_pca_8.shape)\n",
    "\n",
    "test_pca_8 = pca_8.transform(X_test)\n",
    "print(\"Dimension for Test dataset using PCA: \", test_pca_8.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e94fb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_pca_8 = LogisticRegression()\n",
    "logreg_pca_8.fit(train_pca_8, y_train_sm)\n",
    "\n",
    "# making the predictions\n",
    "y_pred_8 = logreg_pca_8.predict(test_pca_8)\n",
    "\n",
    "# converting the prediction into a dataframe\n",
    "y_pred_df_8 = pd.DataFrame(y_pred_8)\n",
    "print(\"Dimension of y_pred_df_8: \", y_pred_df_8.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049745fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the Confusion matrix\n",
    "print(\"Confusion Matirx for y_test & y_pred\\n\",confusion_matrix(y_test,y_pred_8),\"\\n\")\n",
    "\n",
    "# Checking the Accuracy of the Predicted model.\n",
    "print(\"Accuracy of the logistic regression model with PCA: \",accuracy_score(y_test,y_pred_8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
